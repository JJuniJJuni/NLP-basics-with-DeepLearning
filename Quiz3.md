# 1번

## [0805] RNN은 n-gram과 NNLM의 어떠한 단점을 보완한다고 하였나?

1. 연속된 시퀀스의 문자열을 해결할 수 있다
2. 하이퍼 파라미터 연산이 수월하다
3. 고정된 개수의 단어만을 입력으로 받지 않아도 된다
4. 추상화가 잘 되어있어 다른 종류의 모델로 변환하기가 쉽다

# 2번

## [0902] 이 중에 틀린 설명을 고르시오

1. 원-핫 벡터는 단어 벡터간 유의미한 유사도를 계산할 수 없다는 단점이 있다
2. 희소 표현은 '비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다' 라는 가정하에 만들어졌다
3. 논문들을 살펴보면 전반적으로 Skip-gram이 CBOW보다 성능이 좋다라고 알려져있다
4. NNLM과 Word2Vec의 차이점에는 중심단어 예측, 다른 구조 등이 있다

