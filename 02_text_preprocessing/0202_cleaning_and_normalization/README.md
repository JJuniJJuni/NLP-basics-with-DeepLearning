- 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께
1. 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
2. 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.
# 1. 규칙에 기반한 표기가 다른 단어들의 통합
- USA와 US는 같은 의미를 가지므로 하나의 단어로 정규화
# 2. 대, 소문자 통합
- 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법   
ex) 미국을 뜻하는 단어 US와 우리를 뜻하는 us는 구분
- 또 다른 대안은 일부만 소문자로 변환시키는 방법
# 3. 불필요한 단어의 제거
- 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터
## (1) 등장 빈도가 적은 단어
- 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재
## (2) 길이가 짧은 단어
- 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당
- 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법이 크게 유효 X
- 영어 단어와 한국어 단어에서 각 한 글자가 가진 의미의 크기가 다르다는 점에서 기인
- 한국어 단어는 한자어가 많고, 한 글자만으로도 이미 의미를 가진 경우가 많다
```
import re
text = "I was wondering if anyone out there could enlighten me on this car."

# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))

[출력]
was wondering anyone out there could enlighten this car.
```
# 4. 정규 표현식(Regular Expression)
- 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해서 이를 제거할 수 있는 경우가 많음
